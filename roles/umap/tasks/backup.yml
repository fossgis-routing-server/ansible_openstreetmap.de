---
# Borg Backup Tasks for uMap
# This task file handles the deployment of BorgBackup for uMap servers

# Set backup environment variables
- name: Set backup environment variables
  set_fact:
    backup_borg_rsh: "ssh -i /srv/umap/.ssh/{{ umap__backup_ssh_key_name }} -o StrictHostKeyChecking=accept-new -o UserKnownHostsFile=/srv/umap/.ssh/known_hosts"
    backup_data_repo: "ssh://{{ umap__backup_remote_user }}@{{ umap__backup_remote_host }}:{{ umap__backup_remote_port }}/./backups/{{ inventory_hostname }}/umapdata"
    backup_db_repo: "ssh://{{ umap__backup_remote_user }}@{{ umap__backup_remote_host }}:{{ umap__backup_remote_port }}/./backups/{{ inventory_hostname }}/umapdb"
  tags:
    - umap
    - backup

- name: Install borgbackup and dependencies
  apt:
    name:
      - borgbackup
      - jq
    state: present
    update_cache: yes
  tags:
    - umap
    - backup

- name: Create backup group
  group:
    name: borgbackup
    state: present
  tags:
    - umap
    - backup

- name: Add umap user to backup group
  user:
    name: "{{ umap__user }}"
    groups: borgbackup
    append: yes
  tags:
    - umap
    - backup

# Add login users to backup group
# This allows login users (defined in inventory) to run backup commands.
# We filter users based on their group membership (same logic as accounts.yml):
# - Users with group 'all' have access to all hosts
# - Users with groups matching the host's group_names have access
# Note: This filter determines which users SHOULD have access, but doesn't guarantee
# that the user accounts exist on the system.
- name: Gather active accounts for backup group
  set_fact:
    backup_users_active: "{{ backup_users_active | default([]) + [item.0.name] }}"
  when: "item.1 in group_names or item.1 == 'all'"
  loop: "{{ users | default([]) | subelements('groups', skip_missing=True) }}"
  loop_control:
    label: "{{ item.0.name | default('unknown') }}"
  no_log: true  # Suppress output of user data (SSH keys etc.)
  tags:
    - umap
    - backup

- name: Add login users to backup group
  user:
    name: "{{ item }}"
    groups: borgbackup
    append: yes
  loop: "{{ backup_users_active | default([]) }}"
  register: backup_group_result
  ignore_errors: true   # User might not exist
  failed_when: false    # Don't fail playbook if user doesn't exist
  tags:
    - umap
    - backup

- name: Create .ssh directory for umap user
  file:
    path: /srv/umap/.ssh
    state: directory
    owner: "{{ umap__user }}"
    group: borgbackup
    mode: "0750"
  tags:
    - umap
    - backup

- name: Check if SSH key exists on server
  stat:
    path: /srv/umap/.ssh/{{ umap__backup_ssh_key_name }}
  register: ssh_key_stat
  tags:
    - umap
    - backup

# Generate SSH key on server if it doesn't exist
# This allows automatic key generation without manual intervention
- name: Generate SSH key on server if it doesn't exist
  openssh_keypair:
    path: /srv/umap/.ssh/{{ umap__backup_ssh_key_name }}
    type: ed25519
    owner: "{{ umap__user }}"
    group: borgbackup
    mode: "0600"
    comment: "umap-backup-{{ inventory_hostname }}"
  when: not ssh_key_stat.stat.exists
  become: true
  become_user: "{{ umap__user }}"
  register: key_generated
  tags:
    - umap
    - backup

- name: Update ssh_key_stat after generation
  set_fact:
    ssh_key_stat:
      stat:
        exists: true
  when:
    - not ssh_key_stat.stat.exists
    - key_generated.changed | default(false)
  tags:
    - umap
    - backup

# Stop playbook if key was just generated
# User needs to deploy the public key to backup server first
- name: Display deployment command
  debug:
    msg:
      - "SSH-Key wurde auf dem Server generiert: /srv/umap/.ssh/{{ umap__backup_ssh_key_name }}"
      - ""
      - "Bitte deploye den öffentlichen Key jetzt auf den Backupserver:"
      - ""
      - "WICHTIG: Das Deployment muss VOM uMap-Server aus erfolgen, nicht vom Ansible-Host!"
      - ""
      - "1. Führe auf dem umap-Server einen der folgenden Befehle aus:"
      - ""
      - "   Mit persönlichen SSH-Key (muss bereits auf der Storage Box hinterlegt sein):"
      - "   sudo -u umap cat /srv/umap/.ssh/{{ umap__backup_ssh_key_name }}.pub | ssh -p {{ umap__backup_remote_port }} {{ umap__backup_remote_user }}@{{ umap__backup_remote_host }} install-ssh-key"
      - ""
      - "   Hinweis: Der persönliche SSH-Key muss auf dem umap-Server verfügbar sein und auf der Storagebox hinterlegt sein."
      - "   Falls der Key einen anderen Namen hat, Option -i verwenden:"
      - "   sudo -u umap cat /srv/umap/.ssh/{{ umap__backup_ssh_key_name }}.pub | ssh -i ~/.ssh/dein-key-name -p {{ umap__backup_remote_port }} {{ umap__backup_remote_user }}@{{ umap__backup_remote_host }} install-ssh-key"
      - ""
      - "Nach dem Deployment das Playbook erneut ausführen:"
      - "ansible-playbook -l {{ inventory_hostname }} -i hosts.ini site.yml"
  when: key_generated.changed | default(false)
  tags:
    - umap
    - backup

- name: Stop playbook after key generation
  fail:
    msg: "SSH-Key wurde generiert. Bitte deploye den öffentlichen Key auf den Backupserver (siehe Befehl oben) und starte das Playbook erneut."
  when: key_generated.changed | default(false)
  tags:
    - umap
    - backup

- name: Warn if SSH key does not exist
  debug:
    msg: |
      WARNING: SSH key /srv/umap/.ssh/{{ umap__backup_ssh_key_name }} does not exist!
      Backup-related tasks will be skipped.
      The key will be automatically generated on the next playbook run.
  when:
    - not ssh_key_stat.stat.exists
    - not (key_generated.changed | default(false))
  tags:
    - umap
    - backup

- name: Ensure SSH key permissions are correct
  file:
    path: /srv/umap/.ssh/{{ umap__backup_ssh_key_name }}
    state: file
    owner: "{{ umap__user }}"
    group: borgbackup
    mode: "0600"
  when: ssh_key_stat.stat.exists
  tags:
    - umap
    - backup

# Test SSH connection to backup server
# This test is performed after key generation/copying to verify the key is deployed
- name: Test SSH connection to backup server
  command: ssh -i /srv/umap/.ssh/{{ umap__backup_ssh_key_name }} -o ConnectTimeout=5 -o StrictHostKeyChecking=accept-new -o UserKnownHostsFile=/srv/umap/.ssh/known_hosts -p {{ umap__backup_remote_port }} {{ umap__backup_remote_user }}@{{ umap__backup_remote_host }} pwd
  become: true
  become_user: "{{ umap__user }}"
  register: ssh_backup_test
  changed_when: false
  failed_when: false  # Don't treat as error, we check return code instead
  when: ssh_key_stat.stat.exists
  tags:
    - umap
    - backup

# Warn if SSH connection fails (but don't fail the playbook)
# Backup tasks will be skipped, but other tasks continue
- name: Warn if SSH connection to backup server fails
  debug:
    msg: "INFO: Backups sind deaktiviert - SSH-Verbindung zum Backupserver fehlgeschlagen (Key nicht deployt oder Verbindungsproblem)"
  when:
    - ssh_key_stat.stat.exists
    - ssh_backup_test.rc != 0
  tags:
    - umap
    - backup

- name: Ensure script and restore directories exist
  file:
    path: "{{ item.path }}"
    state: directory
    owner: "{{ item.owner }}"
    group: "{{ item.group }}"
    mode: "{{ item.mode }}"
  loop:
    - { path: "{{ umap__paths.host.basedir }}/scripts/backup", owner: "{{ umap__user }}", group: "docker", mode: "0755" }
    - { path: "{{ umap__paths.host.basedir }}/restoredata", owner: "{{ umap__user }}", group: "docker", mode: "0755" }
  tags:
    - umap
    - backup

- name: Deploy backup scripts
  template:
    src: "{{ item.src }}.sh.jinja"
    dest: "{{ umap__paths.host.basedir }}/scripts/backup/{{ item.dest }}"
    owner: "{{ umap__user }}"
    group: docker
    mode: "{{ item.mode }}"
  loop:
    - { src: "backup-ssh-config", dest: "backup-ssh-config.sh", mode: "0644" }
    - { src: "backup-umap-data", dest: "backup-umap-data.sh", mode: "0755" }
    - { src: "backup-umap-db", dest: "backup-umap-db.sh", mode: "0755" }
  when: ssh_key_stat.stat.exists
  tags:
    - umap
    - backup

- name: Deploy backup management tool
  template:
    src: umap-backup.sh.jinja
    dest: "{{ umap__paths.host.basedir }}/scripts/admin/umap-backup.sh"
    owner: "{{ umap__user }}"
    group: docker
    mode: "0755"
  when: ssh_key_stat.stat.exists
  tags:
    - umap
    - backup

- name: Deploy sudoers configuration for borgbackup group
  copy:
    content: |
      # Allow members of borgbackup group to run borg and ssh commands as umap user without password
      # This enables direct access to backup commands without needing explicit sudo
      %borgbackup ALL=({{ umap__user }}) NOPASSWD: /usr/bin/borg, /usr/bin/ssh
    dest: /etc/sudoers.d/borgbackup
    owner: root
    group: root
    mode: "0440"
    validate: /usr/sbin/visudo -cf %s
  tags:
    - umap
    - backup

# Create remote backup directories on backup server
# Note: This command is executed on every Ansible run, even if directories already exist.
# mkdir -p is idempotent (creates only if missing, no error if exists), so this is safe but
# not optimal. The command will always run, but mkdir -p won't do anything if dirs exist.
# changed_when: false prevents false "changed" status since mkdir -p always succeeds.
- name: Create remote backup directories
  command: >
    ssh -i /srv/umap/.ssh/{{ umap__backup_ssh_key_name }}
    -o StrictHostKeyChecking=accept-new
    -o UserKnownHostsFile=/srv/umap/.ssh/known_hosts
    -p {{ umap__backup_remote_port }}
    {{ umap__backup_remote_user }}@{{ umap__backup_remote_host }}
    mkdir -p backups/{{ inventory_hostname }}/{{ item }}
  become: true
  become_user: "{{ umap__user }}"
  loop:
    - umapdata
    - umapdb
  changed_when: false  # mkdir -p is idempotent, always succeeds (no change if dir exists)
  failed_when: false   # Don't fail if directories already exist (mkdir -p handles this)
  ignore_errors: true  # Ignore any other errors (e.g., permission issues)
  when: ssh_backup_test.rc | default(1) == 0  # Only if SSH connection to backup server works
  tags:
    - umap
    - backup

# Check and initialize Borg repositories for data and database
# We use a two-step process:
# 1. Check if repository exists (using borg info)
# 2. Initialize repository only if it doesn't exist (rc != 0 means repository not found)
# We use failed_when: false and ignore_errors: true because borg info returns non-zero
# exit code when repository doesn't exist, which is expected behavior, not an error.
- name: Check if Borg repository exists
  command: borg info {{ repo_item.repo }}
  become: true
  become_user: "{{ umap__user }}"
  environment:
    BORG_RSH: "{{ backup_borg_rsh }}"
    BORG_PASSPHRASE: "{{ umap__backup_passphrase }}"
  register: borg_repo_check
  changed_when: false
  failed_when: false  # borg info returns non-zero if repo doesn't exist (expected)
  ignore_errors: true  # Don't fail the playbook if repo doesn't exist
  when: ssh_backup_test.rc | default(1) == 0  # Only if SSH connection to backup server works
  loop:
    - { name: "data", repo: "{{ backup_data_repo }}" }
    - { name: "db", repo: "{{ backup_db_repo }}" }
  loop_control:
    loop_var: repo_item
  tags:
    - umap
    - backup

# Initialize repository only if it doesn't exist
# Filter repositories that don't exist (rc != 0) before looping to reduce output
- name: Filter repositories that need initialization
  set_fact:
    repos_to_init: "{{ repos_to_init | default([]) + [item] }}"
  when:
    - ssh_backup_test.rc | default(1) == 0  # Only if SSH connection to backup server works
    - borg_repo_check.results is defined
    - item.rc != 0  # Repository doesn't exist
  loop: "{{ borg_repo_check.results | default([]) }}"
  loop_control:
    label: "{{ item.item.name | default('unknown') }} repository"
  no_log: true
  tags:
    - umap
    - backup

- name: Initialize Borg repository if it does not exist
  command: borg init --encryption=repokey {{ item.item.repo }}
  become: true
  become_user: "{{ umap__user }}"
  environment:
    BORG_RSH: "{{ backup_borg_rsh }}"
    BORG_PASSPHRASE: "{{ umap__backup_passphrase }}"
  when:
    - ssh_backup_test.rc | default(1) == 0  # Only if SSH connection to backup server works
    - repos_to_init is defined
    - repos_to_init | length > 0
  failed_when: false  # Don't fail if init fails (might already exist from concurrent run)
  ignore_errors: true
  loop: "{{ repos_to_init }}"
  loop_control:
    label: "{{ item.item.name }} repository"
  tags:
    - umap
    - backup

- name: Deploy systemd service and timer files
  template:
    src: "{{ item }}.jinja"
    dest: "/etc/systemd/system/{{ item }}"
    owner: root
    group: root
    mode: "0644"
  loop:
    - borgbackup-data.service
    - borgbackup-data.timer
    - borgbackup-db.service
    - borgbackup-db.timer
  when: ssh_backup_test.rc | default(1) == 0  # Only if SSH connection to backup server works
  tags:
    - umap
    - backup

# Systemd Timer Management
# Timer state is managed based on SSH key availability and connection status:
# - If SSH key exists AND connection works: Enable and start timers (backups are ready to run)
# - If SSH key is missing OR connection fails: Disable and stop timers (prevent failed backup attempts)
# 
# Why disable timers when connection fails?
# - Backup scripts check for SSH key and exit with code 1 if missing (clean failure)
# - This task prevents repeated failed jobs and makes the state explicit: backups disabled
# - Without this, you'd see failed backup attempts every timer interval
- name: Disable and stop backup timers when SSH key is missing
  systemd:
    name: "{{ item }}"
    daemon_reload: yes
    enabled: no
    state: stopped
  loop:
    - borgbackup-data.timer
    - borgbackup-db.timer
  when: not ssh_key_stat.stat.exists
  ignore_errors: true  # Timers might not exist yet (first run) or already disabled
  failed_when: false
  tags:
    - umap
    - backup

- name: Disable and stop backup timers when SSH connection fails
  systemd:
    name: "{{ item }}"
    daemon_reload: yes
    enabled: no
    state: stopped
  loop:
    - borgbackup-data.timer
    - borgbackup-db.timer
  when:
    - ssh_key_stat.stat.exists
    - ssh_backup_test.rc | default(1) != 0  # Disable if SSH connection to backup server fails
  ignore_errors: true  # Timers might not exist yet (first run) or already disabled
  failed_when: false
  tags:
    - umap
    - backup

- name: Enable and start backup timers
  systemd:
    name: "{{ item }}"
    daemon_reload: yes
    enabled: yes
    state: started
  loop:
    - borgbackup-data.timer
    - borgbackup-db.timer
  when: ssh_backup_test.rc | default(1) == 0  # Only if SSH connection to backup server works
  tags:
    - umap
    - backup

